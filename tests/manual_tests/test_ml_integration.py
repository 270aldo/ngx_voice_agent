#!/usr/bin/env python3
"""
Test de IntegraciÃ³n del Sistema ML Adaptativo en ConversationService

Este script valida que el sistema ML adaptativo estÃ© completamente integrado
y funcionando correctamente en el ConversationService.

Ejecutar: python test_ml_integration.py
"""

import asyncio
import logging
import sys
import json
from datetime import datetime
from pathlib import Path

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# AÃ±adir src al path
sys.path.append(str(Path(__file__).parent / 'src'))

# Importar servicios necesarios
from src.services.conversation_service import ConversationService
from src.models.conversation import CustomerData
from src.models.platform_context import PlatformInfo, SourceType

class MLIntegrationTester:
    """Tester para validar integraciÃ³n ML en ConversationService."""
    
    def __init__(self):
        self.conversation_service = None
        self.test_results = []
        
    async def setup(self):
        """Configurar el test environment."""
        try:
            logger.info("ðŸ”§ Configurando test environment...")
            
            # Inicializar ConversationService
            self.conversation_service = ConversationService(industry='fitness')
            
            # Configurar platform context
            platform_info = PlatformInfo(
                source=SourceType.WEB_WIDGET,
                source_type="lead_magnet",
                campaign_id="test_ml_integration"
            )
            
            logger.info("âœ… Test environment configurado correctamente")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error configurando test environment: {str(e)}")
            return False
    
    async def test_ml_services_initialization(self):
        """Test 1: Verificar que los servicios ML estÃ¡n inicializados."""
        logger.info("ðŸ§ª Test 1: InicializaciÃ³n de servicios ML")
        
        try:
            # Verificar que los servicios ML estÃ¡n inicializados
            assert hasattr(self.conversation_service, 'outcome_tracker'), "OutcomeTracker no inicializado"
            assert hasattr(self.conversation_service, 'adaptive_learning_service'), "AdaptiveLearningService no inicializado"
            assert hasattr(self.conversation_service, 'ab_testing_framework'), "ABTestingFramework no inicializado"
            assert hasattr(self.conversation_service, 'active_experiments'), "Cache de experimentos no inicializado"
            
            logger.info("âœ… Test 1 PASSED: Todos los servicios ML estÃ¡n inicializados")
            self.test_results.append({"test": "ml_services_initialization", "status": "PASSED"})
            return True
            
        except AssertionError as e:
            logger.error(f"âŒ Test 1 FAILED: {str(e)}")
            self.test_results.append({"test": "ml_services_initialization", "status": "FAILED", "error": str(e)})
            return False
        except Exception as e:
            logger.error(f"âŒ Test 1 ERROR: {str(e)}")
            self.test_results.append({"test": "ml_services_initialization", "status": "ERROR", "error": str(e)})
            return False
    
    async def test_conversation_start_with_ml_tracking(self):
        """Test 2: Verificar que el tracking ML se inicia correctamente."""
        logger.info("ðŸ§ª Test 2: Inicio de tracking ML en nueva conversaciÃ³n")
        
        try:
            # Crear datos de cliente de prueba
            customer_data = CustomerData(
                id="test_client_001",
                name="Carlos Testeo",
                age=35,
                interests=["fitness", "productividad"]
            )
            
            # Iniciar conversaciÃ³n
            state = await self.conversation_service.start_conversation(customer_data)
            
            # Verificar que la conversaciÃ³n se creÃ³
            assert state is not None, "Estado de conversaciÃ³n no creado"
            assert state.id is not None, "ID de conversaciÃ³n no asignado"
            
            # Verificar que el tracking ML estÃ¡ activo
            ml_summary = await self.conversation_service.get_ml_conversation_summary(state.id)
            assert ml_summary is not None, "Tracking ML no iniciado"
            
            logger.info(f"âœ… Test 2 PASSED: Tracking ML iniciado para conversaciÃ³n {state.id}")
            self.test_results.append({
                "test": "conversation_start_ml_tracking", 
                "status": "PASSED",
                "conversation_id": state.id
            })
            return state
            
        except Exception as e:
            logger.error(f"âŒ Test 2 FAILED: {str(e)}")
            self.test_results.append({"test": "conversation_start_ml_tracking", "status": "FAILED", "error": str(e)})
            return None
    
    async def test_message_processing_with_ml_updates(self, state):
        """Test 3: Verificar que las mÃ©tricas ML se actualizan durante mensajes."""
        logger.info("ðŸ§ª Test 3: ActualizaciÃ³n de mÃ©tricas ML durante procesamiento")
        
        if not state:
            logger.error("âŒ Test 3 SKIPPED: No hay estado de conversaciÃ³n")
            self.test_results.append({"test": "message_processing_ml_updates", "status": "SKIPPED"})
            return False
        
        try:
            # Procesar un mensaje de prueba
            test_message = "Hola, estoy interesado en mejorar mi rendimiento fÃ­sico y mental"
            
            # Obtener resumen ML antes del mensaje
            ml_before = await self.conversation_service.get_ml_conversation_summary(state.id)
            message_count_before = ml_before.get("message_count", 0) if ml_before else 0
            
            # Procesar mensaje (esto deberÃ­a actualizar mÃ©tricas ML)
            updated_state, audio_response = await self.conversation_service.process_message(
                conversation_id=state.id,
                message_text=test_message
            )
            
            # Obtener resumen ML despuÃ©s del mensaje
            ml_after = await self.conversation_service.get_ml_conversation_summary(state.id)
            
            # Verificar que las mÃ©tricas se actualizaron
            assert ml_after is not None, "Resumen ML no disponible despuÃ©s del mensaje"
            message_count_after = ml_after.get("message_count", 0)
            assert message_count_after > message_count_before, "Contador de mensajes no actualizado"
            
            # Verificar que hay mÃ©tricas actuales
            current_metrics = ml_after.get("current_metrics", {})
            assert len(current_metrics) > 0, "MÃ©tricas actuales no disponibles"
            
            logger.info(f"âœ… Test 3 PASSED: MÃ©tricas ML actualizadas (mensajes: {message_count_before} â†’ {message_count_after})")
            self.test_results.append({
                "test": "message_processing_ml_updates", 
                "status": "PASSED",
                "metrics_before": message_count_before,
                "metrics_after": message_count_after
            })
            return updated_state
            
        except Exception as e:
            logger.error(f"âŒ Test 3 FAILED: {str(e)}")
            self.test_results.append({"test": "message_processing_ml_updates", "status": "FAILED", "error": str(e)})
            return state
    
    async def test_adaptive_learning_status(self):
        """Test 4: Verificar que el estado del sistema adaptativo es accesible."""
        logger.info("ðŸ§ª Test 4: Estado del sistema de aprendizaje adaptativo")
        
        try:
            # Obtener estado del sistema de aprendizaje
            learning_status = await self.conversation_service.get_adaptive_learning_status()
            
            # Verificar que el estado es vÃ¡lido
            assert learning_status is not None, "Estado de aprendizaje no disponible"
            assert isinstance(learning_status, dict), "Estado de aprendizaje no es un diccionario"
            
            # Log del estado para debugging
            logger.info(f"Estado del sistema ML: {json.dumps(learning_status, indent=2, default=str)}")
            
            logger.info("âœ… Test 4 PASSED: Estado del sistema de aprendizaje accesible")
            self.test_results.append({
                "test": "adaptive_learning_status", 
                "status": "PASSED",
                "learning_status": learning_status
            })
            return True
            
        except Exception as e:
            logger.error(f"âŒ Test 4 FAILED: {str(e)}")
            self.test_results.append({"test": "adaptive_learning_status", "status": "FAILED", "error": str(e)})
            return False
    
    async def test_conversation_outcome_recording(self, state):
        """Test 5: Verificar que los outcomes se registran correctamente para ML."""
        logger.info("ðŸ§ª Test 5: Registro de outcome para ML")
        
        if not state:
            logger.error("âŒ Test 5 SKIPPED: No hay estado de conversaciÃ³n")
            self.test_results.append({"test": "conversation_outcome_recording", "status": "SKIPPED"})
            return False
        
        try:
            # Simular un outcome de conversaciÃ³n
            await self.conversation_service._record_conversation_outcome_for_ml(
                state=state,
                outcome_type="test_completed",
                additional_context={
                    "tier_accepted": "PRIME_pro",
                    "satisfaction_score": 8.5,
                    "test_mode": True
                }
            )
            
            logger.info("âœ… Test 5 PASSED: Outcome registrado correctamente para ML")
            self.test_results.append({
                "test": "conversation_outcome_recording", 
                "status": "PASSED",
                "outcome_type": "test_completed"
            })
            return True
            
        except Exception as e:
            logger.error(f"âŒ Test 5 FAILED: {str(e)}")
            self.test_results.append({"test": "conversation_outcome_recording", "status": "FAILED", "error": str(e)})
            return False
    
    def print_test_summary(self):
        """Imprimir resumen de todos los tests."""
        logger.info("\n" + "="*60)
        logger.info("ðŸ“Š RESUMEN DE TESTS ML INTEGRATION")
        logger.info("="*60)
        
        passed_tests = [t for t in self.test_results if t["status"] == "PASSED"]
        failed_tests = [t for t in self.test_results if t["status"] == "FAILED"]
        error_tests = [t for t in self.test_results if t["status"] == "ERROR"]
        skipped_tests = [t for t in self.test_results if t["status"] == "SKIPPED"]
        
        total_tests = len(self.test_results)
        success_rate = (len(passed_tests) / total_tests * 100) if total_tests > 0 else 0
        
        logger.info(f"ðŸ“ˆ TOTAL TESTS: {total_tests}")
        logger.info(f"âœ… PASSED: {len(passed_tests)}")
        logger.info(f"âŒ FAILED: {len(failed_tests)}")
        logger.info(f"ðŸ”¥ ERROR: {len(error_tests)}")
        logger.info(f"â­ï¸  SKIPPED: {len(skipped_tests)}")
        logger.info(f"ðŸŽ¯ SUCCESS RATE: {success_rate:.1f}%")
        
        if failed_tests or error_tests:
            logger.info("\nðŸ” DETALLES DE FALLOS:")
            for test in failed_tests + error_tests:
                logger.error(f"  - {test['test']}: {test.get('error', 'Unknown error')}")
        
        # Determinar resultado final
        if success_rate >= 80.0:
            logger.info("\nðŸŽ‰ RESULTADO: SISTEMA ML ADAPTATIVO INTEGRADO EXITOSAMENTE")
            return True
        else:
            logger.error("\nðŸ’¥ RESULTADO: INTEGRACIÃ“N ML REQUIERE CORRECCIONES")
            return False

async def main():
    """FunciÃ³n principal de testing."""
    logger.info("ðŸš€ Iniciando tests de integraciÃ³n ML adaptativo...")
    
    tester = MLIntegrationTester()
    
    # Setup
    if not await tester.setup():
        logger.error("ðŸ’¥ Setup fallÃ³, abortando tests")
        return False
    
    # Ejecutar tests secuencialmente
    await tester.test_ml_services_initialization()
    
    # Test de conversaciÃ³n (devuelve state para tests siguientes)
    state = await tester.test_conversation_start_with_ml_tracking()
    
    # Tests que dependen del state
    final_state = await tester.test_message_processing_with_ml_updates(state)
    await tester.test_adaptive_learning_status()
    await tester.test_conversation_outcome_recording(final_state)
    
    # Mostrar resumen
    success = tester.print_test_summary()
    
    return success

if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("\nâš¡ Tests interrumpidos por usuario")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ðŸ’¥ Error crÃ­tico ejecutando tests: {str(e)}")
        sys.exit(1)