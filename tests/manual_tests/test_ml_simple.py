#!/usr/bin/env python3
"""
Test Simple del Sistema ML Adaptativo

Prueba b√°sica para validar que los servicios ML est√°n funcionando.
"""

import asyncio
import logging
import sys
from pathlib import Path

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# A√±adir src al path
sys.path.append(str(Path(__file__).parent / 'src'))

async def test_ml_imports():
    """Test b√°sico de importaci√≥n de servicios ML."""
    logger.info("üß™ Testing ML imports...")
    
    try:
        # Test 1: Importar servicios ML b√°sicos
        from src.services.conversation_outcome_tracker import ConversationOutcomeTracker
        logger.info("‚úÖ ConversationOutcomeTracker importado correctamente")
        
        from src.services.adaptive_learning_service import AdaptiveLearningService
        logger.info("‚úÖ AdaptiveLearningService importado correctamente")
        
        from src.services.ab_testing_framework import ABTestingFramework
        logger.info("‚úÖ ABTestingFramework importado correctamente")
        
        from src.models.learning_models import AdaptiveLearningConfig
        logger.info("‚úÖ AdaptiveLearningConfig importado correctamente")
        
        # Test 2: Crear instancias b√°sicas
        tracker = ConversationOutcomeTracker()
        logger.info("‚úÖ ConversationOutcomeTracker instanciado")
        
        learning_service = AdaptiveLearningService(tracker)
        logger.info("‚úÖ AdaptiveLearningService instanciado")
        
        config = AdaptiveLearningConfig()
        ab_framework = ABTestingFramework(config)
        logger.info("‚úÖ ABTestingFramework instanciado")
        
        # Test 3: Verificar m√©todos principales
        learning_summary = learning_service.get_learning_summary()
        assert isinstance(learning_summary, dict), "Learning summary debe ser un diccionario"
        logger.info(f"‚úÖ Learning summary obtenido: {learning_summary}")
        
        # Test 4: Verificar que el tracker tiene los m√©todos necesarios
        assert hasattr(tracker, 'start_tracking_conversation'), "M√©todo start_tracking_conversation faltante"
        assert hasattr(tracker, 'update_conversation_metrics'), "M√©todo update_conversation_metrics faltante"
        assert hasattr(tracker, 'record_conversation_outcome'), "M√©todo record_conversation_outcome faltante"
        logger.info("‚úÖ Todos los m√©todos del tracker est√°n presentes")
        
        # Test 5: Verificar que el AB framework tiene los m√©todos necesarios
        assert hasattr(ab_framework, 'get_active_experiments'), "M√©todo get_active_experiments faltante"
        assert hasattr(ab_framework, 'record_experiment_outcome'), "M√©todo record_experiment_outcome faltante"
        logger.info("‚úÖ Todos los m√©todos del AB framework est√°n presentes")
        
        logger.info("üéâ TODOS LOS TESTS ML B√ÅSICOS PASARON EXITOSAMENTE")
        return True
        
    except ImportError as e:
        logger.error(f"‚ùå Error de importaci√≥n: {str(e)}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Error en tests ML: {str(e)}")
        return False

async def test_ml_conversation_tracker():
    """Test del tracker de conversaciones."""
    logger.info("üß™ Testing ConversationOutcomeTracker...")
    
    try:
        from src.services.conversation_outcome_tracker import ConversationOutcomeTracker
        from src.models.conversation import Message
        from datetime import datetime
        
        # Crear tracker
        tracker = ConversationOutcomeTracker()
        
        # Test datos de cliente simulados
        test_conversation_id = "test_conv_001"
        client_data = {
            "customer_id": "test_customer_001",
            "program_type": "PRIME",
            "age": 35,
            "interests": ["fitness", "productividad"]
        }
        
        # Iniciar tracking
        await tracker.start_tracking_conversation(
            conversation_id=test_conversation_id,
            initial_client_data=client_data,
            experiment_assignments=[]
        )
        logger.info("‚úÖ Tracking iniciado correctamente")
        
        # Obtener resumen
        summary = tracker.get_conversation_summary(test_conversation_id)
        assert summary is not None, "Summary no debe ser None"
        assert summary['conversation_id'] == test_conversation_id, "ID de conversaci√≥n incorrecto"
        logger.info(f"‚úÖ Summary obtenido: {summary}")
        
        # Crear mensaje simulado
        test_message = Message(
            role="user", 
            content="Hola, estoy interesado en el programa PRIME",
            timestamp=datetime.now()
        )
        
        # Actualizar m√©tricas
        await tracker.update_conversation_metrics(
            conversation_id=test_conversation_id,
            message=test_message,
            response_time_seconds=2.5,
            additional_metrics={"emotional_intelligence_score": 0.8}
        )
        logger.info("‚úÖ M√©tricas actualizadas correctamente")
        
        # Obtener summary actualizado
        updated_summary = tracker.get_conversation_summary(test_conversation_id)
        assert updated_summary['message_count'] >= 1, "Contador de mensajes no actualizado"
        logger.info("‚úÖ Summary actualizado correctamente")
        
        logger.info("üéâ CONVERSATION TRACKER TESTS PASARON EXITOSAMENTE")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error en conversation tracker tests: {str(e)}")
        return False

async def main():
    """Funci√≥n principal."""
    logger.info("üöÄ Iniciando tests ML simplificados...")
    
    # Test 1: Imports b√°sicos
    test1_result = await test_ml_imports()
    
    # Test 2: Conversation tracker
    test2_result = await test_ml_conversation_tracker()
    
    # Resumen final
    total_tests = 2
    passed_tests = sum([test1_result, test2_result])
    success_rate = (passed_tests / total_tests) * 100
    
    logger.info("\n" + "="*50)
    logger.info("üìä RESUMEN DE TESTS ML SIMPLIFICADOS")
    logger.info("="*50)
    logger.info(f"üìà TOTAL TESTS: {total_tests}")
    logger.info(f"‚úÖ PASSED: {passed_tests}")
    logger.info(f"‚ùå FAILED: {total_tests - passed_tests}")
    logger.info(f"üéØ SUCCESS RATE: {success_rate:.1f}%")
    
    if success_rate >= 100.0:
        logger.info("\nüéâ RESULTADO: SERVICIOS ML B√ÅSICOS FUNCIONANDO CORRECTAMENTE")
        return True
    else:
        logger.error("\nüí• RESULTADO: ALGUNOS SERVICIOS ML REQUIEREN CORRECCIONES")
        return False

if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("\n‚ö° Tests interrumpidos por usuario")
        sys.exit(1)
    except Exception as e:
        logger.error(f"üí• Error cr√≠tico: {str(e)}")
        sys.exit(1)